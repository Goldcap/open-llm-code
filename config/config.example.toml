# Open LLM Code Configuration Example
# Copy this to ~/.config/open-llm-code/config.toml and customize

[llm]
# Provider: "anthropic", "ollama", or "huggingface"
provider = "huggingface"

# Model name
model = "codellama/CodeLlama-7b-Instruct-hf"

# Environment variable name for API key (Anthropic and HuggingFace)
api_key_env = "HUGGINGFACE_API_KEY"

# Maximum tokens in response
max_tokens = 4096

[ollama]
# Ollama API endpoint
endpoint = "http://localhost:11434"

# Model to use (run `ollama list` to see available models)
model = "codellama:13b"

[huggingface]
# HuggingFace Inference API endpoint
endpoint = "https://api-inference.huggingface.co"

# Model to use (e.g., "codellama/CodeLlama-7b-Instruct-hf")
# Available models: https://huggingface.co/models?pipeline_tag=text-generation
model = "codellama/CodeLlama-7b-Instruct-hf"

[opensearch]
# OpenSearch endpoint URL
endpoint = "https://search-claude-ltm-7m5t3scn2lls4drmfth3jpkfaa.us-west-2.es.amazonaws.com"

# Username for authentication
username = "admin"

# Environment variable name for password
password_env = "OPENSEARCH_PASSWORD"

# Index name for storing sessions
index = "ollm-sessions"

# MCP Servers Configuration
# Add as many servers as you need

[[mcp_servers]]
name = "claude-ltm"
command = "cltm-server"
args = []

[[mcp_servers]]
name = "aws-eks"
command = "npx"
args = ["-y", "@modelcontextprotocol/server-aws-eks"]
[mcp_servers.env]
AWS_REGION = "us-west-2"

[[mcp_servers]]
name = "kubernetes"
command = "npx"
args = ["-y", "@modelcontextprotocol/server-kubernetes"]

[[mcp_servers]]
name = "aws-api"
command = "npx"
args = ["-y", "@modelcontextprotocol/server-aws-api"]
[mcp_servers.env]
AWS_REGION = "us-west-2"

# Example: Custom MCP server
# [[mcp_servers]]
# name = "my-custom-server"
# command = "/path/to/server"
# args = ["--option", "value"]
# [mcp_servers.env]
# API_KEY = "secret"
