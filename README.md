# Open LLM Code (ollm)

A Rust-based AI coding assistant with pluggable LLM backends and MCP (Model Context Protocol) support.

## Features

- ğŸ”Œ **Pluggable LLM Backends**
  - Anthropic Claude API
  - Ollama (CodeLlama, Llama 3, etc.)
  - Easy to add more providers

- ğŸ› ï¸ **MCP Protocol Support**
  - Connect to multiple MCP servers
  - Auto-discover available tools
  - Execute tools seamlessly in conversations

- ğŸ’¾ **Session Persistence**
  - Store conversations in OpenSearch
  - Resume previous sessions
  - Full-text search across history

- ğŸ¨ **Clean Terminal UI**
  - Interactive REPL with history
  - Syntax highlighting
  - Streaming responses

## Installation

### Prerequisites

- Rust 1.70+ (`source /root/.cargo/env`)
- OpenSearch 2.11+ (for session persistence)
- Optional: Ollama (for local models)

### Build from Source

```bash
cd /srv/repos/open-llm-code
cargo build --release

# Install binary
cp target/release/ollm /usr/local/bin/
chmod +x /usr/local/bin/ollm
```

## Quick Start

### 1. Initialize Configuration

```bash
ollm init
```

This creates `~/.config/open-llm-code/config.toml` with example configuration.

### 2. Configure Your Setup

Edit `~/.config/open-llm-code/config.toml`:

```toml
[llm]
provider = "anthropic"  # or "ollama"
model = "claude-sonnet-4"
api_key_env = "ANTHROPIC_API_KEY"
max_tokens = 4096

[ollama]
endpoint = "http://localhost:11434"
model = "codellama:13b"

[opensearch]
endpoint = "https://your-opensearch-domain.amazonaws.com"
username = "admin"
password_env = "OPENSEARCH_PASSWORD"
index = "ollm-sessions"

[[mcp_servers]]
name = "claude-ltm"
command = "cltm-server"

[[mcp_servers]]
name = "aws-eks"
command = "npx"
args = ["-y", "@modelcontextprotocol/server-aws-eks"]
env = { AWS_REGION = "us-west-2" }
```

### 3. Set Environment Variables

```bash
export ANTHROPIC_API_KEY="your-api-key-here"
export OPENSEARCH_PASSWORD="your-opensearch-password"
```

### 4. Start the REPL

```bash
ollm
```

## Usage

### Interactive Mode

```bash
ollm
```

Start a conversation with your configured LLM. The assistant has access to all MCP tools.

### With Specific Config

```bash
ollm --config /path/to/config.toml
```

### Verbose Logging

```bash
ollm --verbose
```

## Architecture

```
ollm
â”œâ”€â”€ LLM Providers (pluggable)
â”‚   â”œâ”€â”€ Anthropic (Claude)
â”‚   â””â”€â”€ Ollama (CodeLlama, etc.)
â”œâ”€â”€ MCP Client
â”‚   â”œâ”€â”€ Protocol implementation
â”‚   â”œâ”€â”€ Tool discovery
â”‚   â””â”€â”€ Tool execution
â”œâ”€â”€ Session Manager
â”‚   â”œâ”€â”€ OpenSearch persistence
â”‚   â””â”€â”€ Conversation history
â””â”€â”€ Terminal UI
    â”œâ”€â”€ REPL with history
    â””â”€â”€ Streaming output
```

## Configuration

### LLM Providers

**Anthropic:**
```toml
[llm]
provider = "anthropic"
model = "claude-sonnet-4"
api_key_env = "ANTHROPIC_API_KEY"
```

**Ollama:**
```toml
[llm]
provider = "ollama"
model = "codellama:13b"

[ollama]
endpoint = "http://localhost:11434"
model = "codellama:13b"
```

### MCP Servers

Add as many MCP servers as needed:

```toml
[[mcp_servers]]
name = "server-name"
command = "command-to-run"
args = ["arg1", "arg2"]
env = { KEY = "value" }
```

## Development

### Project Structure

```
src/
â”œâ”€â”€ main.rs                 # CLI entry point
â”œâ”€â”€ config/                 # Configuration management
â”œâ”€â”€ error.rs                # Error types
â”œâ”€â”€ types.rs                # Core data structures
â”œâ”€â”€ llm/                    # LLM provider implementations
â”‚   â”œâ”€â”€ anthropic.rs
â”‚   â””â”€â”€ ollama.rs
â”œâ”€â”€ mcp/                    # MCP protocol client
â”‚   â”œâ”€â”€ client.rs
â”‚   â”œâ”€â”€ protocol.rs
â”‚   â””â”€â”€ transport.rs
â”œâ”€â”€ session/                # Session persistence
â”‚   â”œâ”€â”€ manager.rs
â”‚   â””â”€â”€ opensearch.rs
â”œâ”€â”€ tools/                  # Tool execution
â”‚   â””â”€â”€ executor.rs
â””â”€â”€ ui/                     # Terminal UI
    â”œâ”€â”€ repl.rs
    â””â”€â”€ renderer.rs
```

### Building

```bash
# Development build
cargo build

# Release build (optimized)
cargo build --release

# Run tests
cargo test

# Run with logging
RUST_LOG=debug cargo run
```

## Roadmap

- [x] Project structure
- [x] Configuration management
- [x] CLI framework
- [ ] LLM provider trait
- [ ] Anthropic provider implementation
- [ ] Ollama provider implementation
- [ ] MCP client implementation
- [ ] Session persistence (OpenSearch)
- [ ] Terminal REPL
- [ ] Tool execution
- [ ] Streaming responses
- [ ] Session history/search
- [ ] Cost tracking
- [ ] Multi-session management

## License

MIT

## Authors

87 Technologies LLC
